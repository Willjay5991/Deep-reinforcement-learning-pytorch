{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, random, pickle, os.path, math, glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import pdb\n",
    "\n",
    "from atari_wrappers import make_atari, wrap_deepmind,LazyFrames\n",
    "from IPython.display import clear_output\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7efd73f6c550>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and wrap the environment\n",
    "env = make_atari('PongNoFrameskip-v4') # only use in no frameskip environment\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True )\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "# env.render()\n",
    "test = env.reset()\n",
    "for i in range(100):\n",
    "    test = env.step(env.action_space.sample())[0]\n",
    "\n",
    "plt.imshow(test._force()[...,0])\n",
    "\n",
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_actions=5):\n",
    "        \"\"\"\n",
    "        Initialize a deep Q-learning network as described in\n",
    "        https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "        Arguments:\n",
    "            in_channels: number of channel of input.\n",
    "                i.e The number of most recent frames stacked together as describe in the paper\n",
    "            num_actions: number of action-value to output, one-to-one correspondence to action in game.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        return self.fc5(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory_Buffer(object):\n",
    "    def __init__(self, memory_size=1000):\n",
    "        self.buffer = []\n",
    "        self.memory_size = memory_size\n",
    "        self.next_idx = 0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        data = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) <= self.memory_size: # buffer not full\n",
    "            self.buffer.append(data)\n",
    "        else: # buffer is full\n",
    "            self.buffer[self.next_idx] = data\n",
    "        self.next_idx = (self.next_idx + 1) % self.memory_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            idx = random.randint(0, self.size() - 1)\n",
    "            data = self.buffer[idx]\n",
    "            state, action, reward, next_state, done= data\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            \n",
    "            \n",
    "        return np.concatenate(states), actions, rewards, np.concatenate(next_states), dones\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent: \n",
    "    def __init__(self, in_channels = 1, action_space = [], USE_CUDA = False, memory_size = 10000, epsilon  = 1, lr = 1e-4):\n",
    "        self.epsilon = epsilon\n",
    "        self.action_space = action_space\n",
    "        self.memory_buffer = Memory_Buffer(memory_size)\n",
    "        self.DQN = DQN(in_channels = in_channels, num_actions = action_space.n)\n",
    "        self.DQN_target = DQN(in_channels = in_channels, num_actions = action_space.n)\n",
    "        self.DQN_target.load_state_dict(self.DQN.state_dict())\n",
    "\n",
    "\n",
    "        self.USE_CUDA = USE_CUDA\n",
    "        if USE_CUDA:\n",
    "            self.DQN = self.DQN.cuda()\n",
    "            self.DQN_target = self.DQN_target.cuda()\n",
    "        self.optimizer = optim.RMSprop(self.DQN.parameters(),lr=lr, eps=0.001, alpha=0.95)\n",
    "\n",
    "    def observe(self, lazyframe):\n",
    "        # from Lazy frame to tensor\n",
    "        state =  torch.from_numpy(lazyframe._force().transpose(2,0,1)[None]/255).float()\n",
    "        if self.USE_CUDA:\n",
    "            state = state.cuda()\n",
    "        return state\n",
    "\n",
    "    def value(self, state):\n",
    "        q_values = self.DQN(state)\n",
    "        return q_values\n",
    "    \n",
    "    def act(self, state, epsilon = None):\n",
    "        \"\"\"\n",
    "        sample actions with epsilon-greedy policy\n",
    "        recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "        \"\"\"\n",
    "        if epsilon is None: epsilon = self.epsilon\n",
    "\n",
    "        q_values = self.value(state).cpu().detach().numpy()\n",
    "        if random.random()<epsilon:\n",
    "            aciton = random.randrange(self.action_space.n)\n",
    "        else:\n",
    "            aciton = q_values.argmax(1)[0]\n",
    "        return aciton\n",
    "    \n",
    "    def compute_td_loss(self, states, actions, rewards, next_states, is_done, gamma=0.99):\n",
    "        \"\"\" Compute td loss using torch operations only. Use the formula above. \"\"\"\n",
    "        actions = torch.tensor(actions).long()    # shape: [batch_size]\n",
    "        rewards = torch.tensor(rewards, dtype =torch.float)  # shape: [batch_size]\n",
    "        is_done = torch.tensor(is_done).bool()  # shape: [batch_size]\n",
    "        \n",
    "        if self.USE_CUDA:\n",
    "            actions = actions.cuda()\n",
    "            rewards = rewards.cuda()\n",
    "            is_done = is_done.cuda()\n",
    "\n",
    "        # get q-values for all actions in current states\n",
    "        predicted_qvalues = self.DQN(states)\n",
    "\n",
    "        # select q-values for chosen actions\n",
    "        predicted_qvalues_for_actions = predicted_qvalues[\n",
    "          range(states.shape[0]), actions\n",
    "        ]\n",
    "\n",
    "        # compute q-values for all actions in next states\n",
    "        predicted_next_qvalues = self.DQN_target(next_states) # YOUR CODE\n",
    "\n",
    "        # compute V*(next_states) using predicted next q-values\n",
    "        next_state_values =  predicted_next_qvalues.max(-1)[0] # YOUR CODE\n",
    "\n",
    "        # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "        target_qvalues_for_actions = rewards + gamma *next_state_values # YOUR CODE\n",
    "\n",
    "        # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "        target_qvalues_for_actions = torch.where(\n",
    "            is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "        # mean squared error loss to minimize\n",
    "        #loss = torch.mean((predicted_qvalues_for_actions -\n",
    "        #                   target_qvalues_for_actions.detach()) ** 2)\n",
    "        loss = F.smooth_l1_loss(predicted_qvalues_for_actions, target_qvalues_for_actions.detach())\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def sample_from_buffer(self, batch_size):\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            idx = random.randint(0, self.memory_buffer.size() - 1)\n",
    "            data = self.memory_buffer.buffer[idx]\n",
    "            frame, action, reward, next_frame, done= data\n",
    "            states.append(self.observe(frame))\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(self.observe(next_frame))\n",
    "            dones.append(done)\n",
    "        return torch.cat(states), actions, rewards, torch.cat(next_states), dones\n",
    "\n",
    "    def learn_from_experience(self, batch_size):\n",
    "        if self.memory_buffer.size() > batch_size:\n",
    "            states, actions, rewards, next_states, dones = self.sample_from_buffer(batch_size)\n",
    "            td_loss = self.compute_td_loss(states, actions, rewards, next_states, dones)\n",
    "            self.optimizer.zero_grad()\n",
    "            td_loss.backward()\n",
    "            for param in self.DQN.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            return(td_loss.item())\n",
    "        else:\n",
    "            return(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2254440d4adb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_by_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mstate_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mnext_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7e413a8dc6b7>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0maciton\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-7e413a8dc6b7>\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-fda4f49ff3f3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "    \n",
    "# Training DQN in PongNoFrameskip-v4 \n",
    "env = make_atari('PongNoFrameskip-v4')\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True)\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon_max = 1\n",
    "epsilon_min = 0.01\n",
    "eps_decay = 30000\n",
    "frames = 2000000\n",
    "USE_CUDA = True\n",
    "learning_rate = 2e-4\n",
    "max_buff = 100000\n",
    "update_tar_interval = 1000\n",
    "batch_size = 32\n",
    "print_interval = 1000\n",
    "log_interval = 1000\n",
    "learning_start = 10000\n",
    "win_reward = 18     # Pong-v4\n",
    "win_break = True\n",
    "\n",
    "action_space = env.action_space\n",
    "action_dim = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "state_channel = env.observation_space.shape[2]\n",
    "agent = DQNAgent(in_channels = state_channel, action_space= action_space, USE_CUDA = USE_CUDA, lr = learning_rate)\n",
    "\n",
    "frame = env.reset()\n",
    "\n",
    "episode_reward = 0\n",
    "all_rewards = []\n",
    "losses = []\n",
    "episode_num = 0\n",
    "is_win = False\n",
    "# tensorboard\n",
    "#summary_writer = SummaryWriter(log_dir = \"DQN_stackframe\", comment= \"good_makeatari\")\n",
    "\n",
    "# e-greedy decay\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_min + (epsilon_max - epsilon_min) * math.exp(\n",
    "            -1. * frame_idx / eps_decay)\n",
    "# plt.plot([epsilon_by_frame(i) for i in range(10000)])\n",
    "\n",
    "for i in range(frames):\n",
    "    epsilon = epsilon_by_frame(i)\n",
    "    state_tensor = agent.observe(frame)\n",
    "    action = agent.act(state_tensor, epsilon)\n",
    "    \n",
    "    next_frame, reward, done, _ = env.step(action)\n",
    "    \n",
    "    episode_reward += reward\n",
    "    agent.memory_buffer.push(frame, action, reward, next_frame, done)\n",
    "    frame = next_frame\n",
    "    \n",
    "    loss = 0\n",
    "    if agent.memory_buffer.size() >= learning_start:\n",
    "        loss = agent.learn_from_experience(batch_size)\n",
    "        losses.append(loss)\n",
    "\n",
    "    if i % print_interval == 0:\n",
    "        print(\"frames: %5d, reward: %5f, loss: %4f, epsilon: %5f, episode: %4d\" % (i, np.mean(all_rewards[-10:]), loss, epsilon, episode_num))\n",
    "#  #       summary_writer.add_scalar(\"Temporal Difference Loss\", loss, i)\n",
    "#         summary_writer.add_scalar(\"Mean Reward\", np.mean(all_rewards[-10:]), i)\n",
    "#         summary_writer.add_scalar(\"Epsilon\", epsilon, i)\n",
    "        \n",
    "    if i % update_tar_interval == 0:\n",
    "        agent.DQN_target.load_state_dict(agent.DQN.state_dict())\n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        frame = env.reset()\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "        episode_num += 1\n",
    "        avg_reward = float(np.mean(all_rewards[-100:]))\n",
    "\n",
    "#summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(frame_idx, rewards, losses):\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()\n",
    "\n",
    "plot_training(i, all_rewards, losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
