{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## double DQN\n",
    "传统的DQN 存在会高估action的Q值的问题，并且这种高估会随着action的个数的增长而变大。\n",
    "\n",
    "double DQN 可以一定程度上克服这一点\n",
    "#### original DQN\n",
    "$${TargetQ{\\rm = r + }}\\gamma \\mathop {{\\rm{max}}}\\limits_{a'} Q(s',a'|{\\theta ^{tar}})$$\n",
    "$${evalQ{\\rm = }}Q(s,a|{\\theta ^{eval}})$$\n",
    "$$L({\\theta ^{eval}}) = abs(evalQ - TargetQ)$$\n",
    "#### double DQN \n",
    "$$TargetQ{\\rm{ = r + }}\\gamma Q(s',\\mathop {\\max }\\limits_{a'} Q(s',a'|{\\theta ^{eval}})|{\\theta ^{tar}})$$\n",
    "$$evalQ{\\rm{ = }}Q(s,a|{\\theta ^{eval}})$$\n",
    "$$L({\\theta ^{eval}}) = abs(evalQ - TargetQ)$$\n",
    "\n",
    "${\\theta ^{eval}}$ 是evalnet的参数，${\\theta ^{tar}}$ 是targetnet 的参数\n",
    "\n",
    "由上述公式可以看出，Double DQN 和 original DQN 不一样的地方就在于在计算targetQ时，original DQN 使用贪婪方法每个都取targetnet输出最大值对应的action，而double DQN 则选择 evalnet 输出最大值对应的action（这样选出来的action不一定是targetnet输出q值最大的action），然后在计算对应的targetQ。\n",
    "\n",
    "## reference：\n",
    "https://blog.csdn.net/u013236946/article/details/73161586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from atari_wrappers import make_atari, wrap_deepmind,LazyFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "def prepro(obs):\n",
    "    \"\"\"\n",
    "    preprocess observation \n",
    "    [84,84,4]=>[4,84,84]\n",
    "    \"\"\"\n",
    "    obs = obs._force().transpose(2,0,1)/255  # 0-255 => 0-1, [84,84,4] => [4,84,84]\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net\n",
    "import torch\n",
    "from torch import nn\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Initialize a deep Q-learning network as described in\n",
    "    https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "    Arguments:\n",
    "        in_channels: number of channel of input.\n",
    "            i.e The number of most recent frames stacked together as describe in the paper\n",
    "        out_num: number of action-value to output, one-to-one correspondence to action in game.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel=4, out_num=6):\n",
    "        super(Net,self).__init__()\n",
    "        # layer setting paramters\n",
    "        layers = [32,64,64]\n",
    "        # output_size = (input_size-kernel_size)/stride + 1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel,layers[0],kernel_size=8,stride=4), # [batch,4,84,84]=>[batch,layers[0],20,20] \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(layers[0],layers[1],kernel_size=4,stride=2), # [batch,layers[0],20,20]=>[batch,layers[1],9,9]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(layers[1],layers[2],kernel_size=3,stride=1), # [batch,layers[1],9,9]=>[batch,layers[2],7,7]\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.linear1 = nn.Sequential(\n",
    "            nn.Linear(7*7*layers[2], 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.linear2 = nn.Linear(512, out_num)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        # flatten\n",
    "        x = x.reshape(x.size(0), -1)  # [batch, layers[2],7,7]=>[batch,layers[2]*7*7]\n",
    "        x = self.linear1(x)\n",
    "        y = self.linear2(x)\n",
    "        # y = F.softmax(y)  # turn to probability\n",
    "        return y\n",
    "pre_net = Net()\n",
    "print(pre_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree():\n",
    "    data_point = 0  # data idx\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity  # num of leaves\n",
    "        self.tree = np.zeros(capacity*2-1)\n",
    "        self.transition = np.zeros(capacity, dtype=object) # save data\n",
    "        self.n_entries = 0\n",
    "        \n",
    "    def total_p(self):\n",
    "        return self.tree[0]  # return the 0 node\n",
    "        \n",
    "    def add(self, p, data):\n",
    "        # add data and priority\n",
    "        # p: priority, data \n",
    "        idx = self.data_point + self.capacity -1\n",
    "        self.transition[self.data_point] = data   # save data        \n",
    "        self.data_point += 1\n",
    "        if self.data_point >= self.capacity:\n",
    "            self.data_point = 0            \n",
    "        \n",
    "        change = p - self.tree[idx]   \n",
    "        self.tree[idx] = p            # save priority\n",
    "        self.n_entries += 1   # count num of entries\n",
    "        \n",
    "        # propagation  update parents nodes\n",
    "        p_idx = (idx-1)//2  # parent idx\n",
    "        while True:           \n",
    "            self.tree[p_idx] += change\n",
    "            if p_idx != 0:\n",
    "                p_idx = (p_idx-1)//2\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    def update(self,idx, p):\n",
    "        # update tree\n",
    "        change = p - self.tree[idx]\n",
    "        self.tree[idx] = p  # change priority in leaves node        \n",
    "        # update parents node\n",
    "        p_idx = (idx-1)//2   #\n",
    "        while True:            \n",
    "            self.tree[p_idx] += change\n",
    "            if p_idx != 0:\n",
    "                p_idx = (p_idx-1)//2\n",
    "            else:\n",
    "                break\n",
    "    \n",
    "    def retrieve(self, p_idx, s):\n",
    "        # search leaf idx for s        \n",
    "        while True:\n",
    "            l_idx = p_idx*2 + 1  # left child idx\n",
    "            r_idx = l_idx +1     # right child idx  \n",
    "            if l_idx >= self.capacity*2-1: # 其子节点超出，结点序号范围， 说明已经是叶子结点\n",
    "                return p_idx \n",
    "            if self.tree[l_idx] >= s:\n",
    "                p_idx = l_idx\n",
    "            else:\n",
    "                p_idx = r_idx\n",
    "                s -= self.tree[l_idx]\n",
    "            \n",
    "    def get(self, s):\n",
    "        # get a data and priority from tree and transition        \n",
    "        # get idx of leaves\n",
    "        idx = self.retrieve(0,s)\n",
    "        priority = self.tree[idx]\n",
    "        data_idx = idx - (self.capacity - 1)\n",
    "        data = self.transition[data_idx]\n",
    "        return idx, priority, data        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory buffer replay\n",
    "class Memory_buffer_replay():\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "    abs_err_upper = 1.\n",
    "    def __init__(self, capacity, device):\n",
    "        self.capacity = capacity\n",
    "        self.sumtree = SumTree(self.capacity)\n",
    "        self.device = device\n",
    "        \n",
    "        self.e = 0.01\n",
    "        self.a = 0.6\n",
    "        self.prio_max = 0.1\n",
    "        \n",
    "    def store_record(self, state, state_next, a, r, done):\n",
    "        data = (state, state_next, a, r, done)\n",
    "        priority = (np.abs(self.prio_max) + self.e) ** self.a #  proportional priority\n",
    "        self.sumtree.add(priority, data) # save p, data in sumtree\n",
    "        \n",
    "    def sample(self, batchsz):\n",
    "        states, state_nexts, a_s, r_s, done_s = [],[],[],[],[]\n",
    "        idxs, ps, isweights = [],[],[]  # idss, prioritys\n",
    "        segment = self.sumtree.total_p()/batchsz\n",
    "        \n",
    "        self.beta = np.min([1., self.beta+self.beta_increment_per_sampling])\n",
    "        min_prob = np.min(self.sumtree.tree[-self.capacity:])/self.sumtree.total_p()\n",
    "        \n",
    "        for i in range(batchsz):\n",
    "            down_bound = i*segment\n",
    "            up_bound = (i+1)*segment\n",
    "            s = np.random.uniform(down_bound, up_bound)\n",
    "            idx, p, data = self.sumtree.get(s) \n",
    "            \n",
    "            state, state_next, a, r, done = data\n",
    "            prob = p/self.sumtree.total_p()\n",
    "            isweight = np.power(prob/min_prob, -self.beta)\n",
    "            \n",
    "            isweights.append(isweight)\n",
    "            idxs.append(idx)\n",
    "            ps.append(p)\n",
    "            states.append(state)\n",
    "            state_nexts.append(state_next)\n",
    "            a_s.append(a)\n",
    "            r_s.append(r)\n",
    "            done_s.append(done)\n",
    "        \n",
    "        #from list to numpy\n",
    "#         idxs, ps = np.array(idxs), np.array(ps)\n",
    "        # function: list=> tensor\n",
    "        list2tensor = lambda data: torch.from_numpy(np.array(data)).to(self.device)\n",
    "        \n",
    "        isweights = list2tensor(isweights)\n",
    "        states, state_nexts = list2tensor(states), list2tensor(state_nexts)\n",
    "        a_s, r_s, done_s = list2tensor(a_s), list2tensor(r_s), list2tensor(done_s)\n",
    "        # isweights: tensor, idx:list, ps:list, states:tensor, as, rs, done_s:tensor\n",
    "        return isweights, idxs, ps, states, state_nexts, a_s, r_s, done_s\n",
    "    \n",
    "    def update(self, idxs, errors):\n",
    "        # update data priority\n",
    "        self.prio_max = max(self.prio_max, max(np.abs(errors)))\n",
    "        for i, idx in enumerate(idxs):\n",
    "            p = (np.abs(errors[i]) + self.e) ** self.a\n",
    "            self.sumtree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN with priority\n",
    "import torch.nn.functional as F\n",
    "class DDQN_priority():\n",
    "    def __init__(self, args, env, load_model=False):        \n",
    "        # use cuda\n",
    "        cuda = True\n",
    "        if cuda:\n",
    "            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            \n",
    "        # env\n",
    "        self.env = env\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.dim_shape = env.observation_space.shape\n",
    "        \n",
    "        # hyperparams\n",
    "        self.capacity = 100000\n",
    "        self.decay = 0.99\n",
    "        self.episodes = args.episodes\n",
    "        self.log_interval = args.log_interval  # print state per log_interval epsisode\n",
    "        self.save_interval = args.save_interval  # interval for save model \n",
    "        \n",
    "        self.epilon_max = 1\n",
    "        self.epilon_min = 0.01\n",
    "        self.learning_start = 10000  # 装满10000 再开始\n",
    "        self.epilon_decay = 30000\n",
    "        \n",
    "        # learn params\n",
    "        self.lr = args.lr\n",
    "        self.batchsz = args.batchsz\n",
    "        self.iter =0\n",
    "        self.iter_max = args.iter_max\n",
    "        \n",
    "        # build model\n",
    "        self.eval_model = Net().to(self.device)\n",
    "        self.tar_model = Net().to(self.device)  \n",
    "        # load model\n",
    "        if load_model:\n",
    "            self.eval_model.load_state_dict(torch.load('dqn_priority.mdl'))\n",
    "        self.tar_model.load_state_dict(self.eval_model.state_dict())\n",
    "        # build optimizer\n",
    "        self.optimizer = optim.RMSprop(self.eval_model.parameters(),lr=self.lr, eps=0.001, alpha=0.95)\n",
    "#         self.optimizer = optim.Adam(self.eval_model.parameters(), self.lr)\n",
    "#         self.cost_F = nn.MSELoss()\n",
    "        \n",
    "        # build memory buffer\n",
    "        self.buffer = Memory_buffer_replay(self.capacity, self.device)\n",
    "    \n",
    "    def train(self):\n",
    "        episodes = self.episodes\n",
    "        losses = []\n",
    "        rewards  = []\n",
    "        \n",
    "        # decay function explot ratio\n",
    "        epilon_by_g_step = lambda step_idx:self.epilon_min + (self.epilon_max-self.epilon_min)*np.exp(-1*step_idx/self.epilon_decay)\n",
    "        # global step\n",
    "        global_step = 0\n",
    "        \n",
    "        for epis in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            state = prepro(state) # [84,84,4]=>[4,84,84]\n",
    "            reward_sum = 0\n",
    "            loss_ = []\n",
    "            # decay explot ratio\n",
    "#             self.explot_ep = self.explot_ep_min + (self.explot_ep_max-self.explot_ep_min)*np.exp(-1*epis/50.)\n",
    "            while True:\n",
    "                # decay explot ratio\n",
    "                epsilon = epilon_by_g_step(global_step)\n",
    "                global_step += 1 \n",
    "                act = self.make_action(state, epsilon) # choose action\n",
    "                state_next, r, done, inf = self.env.step(act)\n",
    "                state_next = prepro(state_next)\n",
    "                reward_sum += r                \n",
    "                # store data\n",
    "                self.buffer.store_record(state, state_next, act, r, done)                \n",
    "                if done:                    \n",
    "                    rewards.append(reward_sum)\n",
    "                    losses.append(np.mean(loss_))\n",
    "                    break  # start another episode\n",
    "                else:\n",
    "                    state = state_next\n",
    "            \n",
    "                # learn model\n",
    "                if self.buffer.sumtree.n_entries == self.learning_start:\n",
    "                    print('#'*30 + 'start learning' + '#'*30)\n",
    "                if self.buffer.sumtree.n_entries > self.learning_start:\n",
    "                    loss = self.learn()\n",
    "                    loss_.append(loss)\n",
    "                else:\n",
    "                    loss_.append(0)\n",
    "                \n",
    "            # print train state\n",
    "            if epis%self.log_interval==0 and epis>0:\n",
    "                #print(losses)\n",
    "                print('global step:{}'.format(global_step-1),\n",
    "                      'episode :{}/{}'.format(epis, self.episodes), \n",
    "                      'aver loss:{:.5f}'.format(np.mean(losses[-10:])), \n",
    "                      'aver reward:{:.5f}'.format(np.mean(rewards[-10:])),\n",
    "                      'explot:{:.5f}'.format(epsilon)\n",
    "                     )\n",
    "            # save model\n",
    "            if epis%self.save_interval==0 and epis>0:\n",
    "                torch.save(self.eval_model.state_dict(),'dqn_priority.mdl')\n",
    "        return losses, rewards\n",
    "    \n",
    "    def learn(self):\n",
    "        # learn the model\n",
    "        \n",
    "        # get data,  idxs, ps : list. others tensor\n",
    "        isweights, idxs, ps, state, state_next, act_s, r_s, done_s = self.buffer.sample(self.batchsz)        \n",
    "        \n",
    "        # pre\n",
    "        eval_q = self.eval_model(state.float()).gather(1,act_s.unsqueeze(1).long())  # get pre and choose Q with act_c,[batch,1]\n",
    "        eval_q = eval_q.squeeze(-1)  # [batch,1] => [batch]\n",
    "        \n",
    "        ####################### part different with original DQN #######################\n",
    "        # compute label\n",
    "        next_q = self.tar_model(state_next.float()).detach() # not update so use detach\n",
    "        # choose action from eval_model\n",
    "        next_q_eval = self.eval_model(state_next.float()).detach()\n",
    "        action_eval = next_q_eval.max(1)[1]  # return idx of max Q, action:=idx\n",
    "        # choose q with chosen action\n",
    "        next_max_q = next_q.gather(1,action_eval.unsqueeze(1).long())  \n",
    "        next_max_q = next_max_q.squeeze(-1)\n",
    "        ################################################################################\n",
    "        \n",
    "        decay = torch.tensor(self.decay).float().to(self.device)\n",
    "        tar_q = r_s.float() + decay*next_max_q\n",
    "        tar_q = torch.where(done_s>0, r_s.float(), tar_q)  # Q_next = r when done is true\n",
    "        \n",
    "        #compute td-loss\n",
    "#         loss = F.smooth_l1_loss(eval_q, tar_q)\n",
    "        loss = torch.mean(isweights*(eval_q - tar_q)**2) # loss expect input dim=[batch,1]\n",
    "        # update buffer with error\n",
    "        errors = (eval_q - tar_q).detach().cpu().tolist()\n",
    "        self.buffer.update(idxs, errors)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # clip gradient\n",
    "        for param in self.eval_model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # update tar_model\n",
    "        self.iter += 1\n",
    "        if self.iter%self.iter_max==0:\n",
    "            self.tar_model.load_state_dict(self.eval_model.state_dict())\n",
    "        \n",
    "        return loss.cpu().item() # ruturn mean loss\n",
    "    \n",
    "    def make_action(self, state, epsilon=None):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        \n",
    "        pre_q = self.eval_model(state.unsqueeze(0).to(self.device))\n",
    "        pre_q = pre_q.cpu().detach()\n",
    "        \n",
    "        #explot_ep = self.explot_ep_min + (self.explot_ep_max-self.explot_ep_min)*np.exp(-1*epis/20.)\n",
    "        if epsilon==None:\n",
    "            epsilon=self.epsion_min\n",
    "        if np.random.uniform()< epsilon:\n",
    "            action = np.random.choice(self.n_actions) # random choose action\n",
    "        else:\n",
    "            action = torch.argmax(pre_q)\n",
    "            action = action.item()\n",
    "        return action     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argument\n",
    "class Argument():\n",
    "    def __init__(self):\n",
    "        self.lr = 1e-3\n",
    "        self.batchsz = 32\n",
    "        \n",
    "        self.episodes = 210\n",
    "        \n",
    "        self.log_interval = 1 # print state per log_interval epsisode\n",
    "        self.save_interval = 20  # interval for save model\n",
    "        \n",
    "        self.iter_max = 1000  # update tar_model interval\n",
    "\n",
    "args = Argument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "import gym\n",
    "env = make_atari('PongNoFrameskip-v4') # only use in no frameskip environment\n",
    "env = wrap_deepmind(env, scale = False, frame_stack=True )\n",
    "\n",
    "load_model = False\n",
    "run_dqn = DDQN_priority(args, env, load_model)\n",
    "losses, rewards = run_dqn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and rewards\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_(losses, rewards):\n",
    "    plt.figure()\n",
    "    plt.title('rewards')\n",
    "    plt.plot(list(range(len(rewards))), rewards)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title('loss')\n",
    "    plt.plot(list(range(len(losses))),losses)\n",
    "    plt.show()\n",
    "\n",
    "plot_(losses, rewards) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
